{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae14bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and setup\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import hmac\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import findspark\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3999eb7-4c5f-4dde-a0b7-dd393cc5647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load environment variables\n",
    "load_dotenv(dotenv_path=\"/home/jovyan/.env\", override=True)\n",
    "API_KEY = os.getenv(\"BINANCE_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"BINANCE_SECRET_KEY\")\n",
    "\n",
    "print(\"Environment variables loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1e69d3-883e-4716-8b39-2a5b02c159a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1beae8b1-9bf6-4c1c-a815-4f801afa502f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.1.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.1.1/mongo-spark-connector_2.12-10.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.1.1!mongo-spark-connector_2.12.jar (160ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (138ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (280ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (520ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (75ms)\n",
      ":: resolution report :: resolve 3106ms :: artifacts dl 1184ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.1.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1beae8b1-9bf6-4c1c-a815-4f801afa502f\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (2354kB/11ms)\n",
      "25/06/09 15:04:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully\n",
      "Spark version: 3.4.2\n"
     ]
    }
   ],
   "source": [
    "# initialise Spark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BinanceToMongoDB_Enhanced\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \n",
    "            \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \n",
    "            \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d3fcfd-dd79-4ec0-9707-c07cb0ede983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced Binance API functions with authentication\n",
    "def create_signature(query_string, secret_key):\n",
    "    \"\"\"Create HMAC SHA256 signature for authenticated requests\"\"\"\n",
    "    return hmac.new(\n",
    "        secret_key.encode('utf-8'),\n",
    "        query_string.encode('utf-8'),\n",
    "        hashlib.sha256\n",
    "    ).hexdigest()\n",
    "\n",
    "def get_authenticated_klines(symbol=\"BTCUSDT\", interval=\"1h\", limit=1000, start_time=None, end_time=None):\n",
    "    \"\"\"Fetch klines with authenticated API for potentially more data\"\"\"\n",
    "    base_url = \"https://api.binance.com\"\n",
    "    endpoint = \"/api/v3/klines\"\n",
    "    \n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"limit\": min(limit, 1000)  # Binance max is 1000\n",
    "    }\n",
    "    \n",
    "    if start_time:\n",
    "        params[\"startTime\"] = int(start_time)\n",
    "    if end_time:\n",
    "        params[\"endTime\"] = int(end_time)\n",
    "    \n",
    "    # add timestamp for authenticated request\n",
    "    params[\"timestamp\"] = int(time.time() * 1000)\n",
    "    # create query string\n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    # create signature\n",
    "    signature = create_signature(query_string, SECRET_KEY)\n",
    "    # add signature to params\n",
    "    params[\"signature\"] = signature\n",
    "\n",
    "    headers = {\n",
    "        \"X-MBX-APIKEY\": API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url + endpoint, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Authenticated request failed, trying public: {e}\")\n",
    "        # fallback to public API\n",
    "        return get_public_klines(symbol, interval, min(limit, 1000), start_time, end_time)\n",
    "\n",
    "def get_public_klines(symbol=\"BTCUSDT\", interval=\"1h\", limit=1000, start_time=None, end_time=None):\n",
    "    \"\"\"Fallback public API function\"\"\"\n",
    "    url = \"https://api.binance.com/api/v3/klines\"\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"limit\": limit,\n",
    "    }\n",
    "    if start_time:\n",
    "        params[\"startTime\"] = int(start_time)\n",
    "    if end_time:\n",
    "        params[\"endTime\"] = int(end_time)\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d832d154-6a3a-447d-810f-d296671c9d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic fetcher function defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_historical_data(symbol=\"BTCUSDT\", target_count=3000):\n",
    "    \"\"\"\n",
    "    Strategic approach to get 3000+ records:\n",
    "    1. Try shorter intervals first (more data points)\n",
    "    2. Go back in time systematically\n",
    "    3. Use both authenticated and public APIs\n",
    "    \"\"\"\n",
    "    intervals_to_try = [\n",
    "        (\"1m\", \"1-minute\", 1000 * 60),      # 1000 mins back\n",
    "        (\"3m\", \"3-minute\", 3000 * 60),      # 3000 mins back  \n",
    "        (\"5m\", \"5-minute\", 5000 * 60),      # 5000 mins back\n",
    "        (\"15m\", \"15-minute\", 15000 * 60),   # 15000 mins back\n",
    "        (\"30m\", \"30-minute\", 30000 * 60),   # 30000 mins back\n",
    "        (\"1h\", \"1-hour\", 60000 * 60),       # 60000 mins back\n",
    "    ]\n",
    "    \n",
    "    all_rows = []\n",
    "    successful_config = None\n",
    "    for interval, desc, lookback_minutes in intervals_to_try:\n",
    "        print(f\"\\n=== Trying {symbol} with {interval} ({desc}) ===\")\n",
    "        try:\n",
    "            end_time = int(time.time() * 1000)  # current time in milliseconds\n",
    "            lookback_ms = lookback_minutes * 1000  # convert to milliseconds\n",
    "            current_rows = []\n",
    "            current_time = end_time\n",
    "            fetched_batches = 0\n",
    "            max_batches = 10  # limit to avoid infinite loops\n",
    "            while len(current_rows) < target_count and fetched_batches < max_batches:\n",
    "                # calculate start time for this batch\n",
    "                batch_start = current_time - (1000 * lookback_ms // 1000)  # go back by interval\n",
    "                print(f\"Batch {fetched_batches + 1}: Fetching from {datetime.fromtimestamp(batch_start/1000)} to {datetime.fromtimestamp(current_time/1000)}\")\n",
    "                # attempt authenticated first, then public\n",
    "                try:\n",
    "                    data = get_authenticated_klines(\n",
    "                        symbol=symbol, \n",
    "                        interval=interval, \n",
    "                        limit=1000,\n",
    "                        start_time=batch_start,\n",
    "                        end_time=current_time\n",
    "                    )\n",
    "                except:\n",
    "                    data = get_public_klines(\n",
    "                        symbol=symbol, \n",
    "                        interval=interval, \n",
    "                        limit=1000,\n",
    "                        start_time=batch_start,\n",
    "                        end_time=current_time\n",
    "                    )\n",
    "                if not data or len(data) == 0:\n",
    "                    print(f\"No data received for this time range\")\n",
    "                    break\n",
    "                # convert to rows\n",
    "                for row in data:\n",
    "                    current_rows.append(Row(\n",
    "                        symbol=symbol,\n",
    "                        open_time=int(row[0]),\n",
    "                        open=float(row[1]),\n",
    "                        high=float(row[2]),\n",
    "                        low=float(row[3]),\n",
    "                        close=float(row[4]),\n",
    "                        volume=float(row[5]),\n",
    "                        close_time=int(row[6]),\n",
    "                        quote_volume=float(row[7]),\n",
    "                        num_trades=int(row[8]),\n",
    "                        taker_base_volume=float(row[9]),\n",
    "                        taker_quote_volume=float(row[10]),\n",
    "                        ignore=row[11]\n",
    "                    ))\n",
    "                \n",
    "                print(f\"Received {len(data)} records, total: {len(current_rows)}\")\n",
    "                # move time window back\n",
    "                current_time = batch_start - 1\n",
    "                fetched_batches += 1\n",
    "                # rate limiting\n",
    "                time.sleep(0.2) \n",
    "                # stop if we got less than requested (hit the limit)\n",
    "                if len(data) < 1000:\n",
    "                    print(f\"Reached historical data limit\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Final count for {interval}: {len(current_rows)} records\")\n",
    "            # use configuration (with enough records)\n",
    "            if len(current_rows) >= target_count:\n",
    "                print(f\"SUCCESS! Got {len(current_rows)} records with {symbol} {interval}\")\n",
    "                all_rows = current_rows\n",
    "                successful_config = (symbol, interval, desc)\n",
    "                break\n",
    "                \n",
    "            elif len(current_rows) > len(all_rows):\n",
    "                all_rows = current_rows\n",
    "                successful_config = (symbol, interval, desc)\n",
    "                print(f\"Best so far: {len(current_rows)} records with {symbol} {interval}\")      \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol} {interval}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # remove duplicates based on open_time (regarding overlapping batches)\n",
    "    if all_rows:\n",
    "        seen_times = set()\n",
    "        unique_rows = []\n",
    "        for row in all_rows:\n",
    "            if row.open_time not in seen_times:\n",
    "                unique_rows.append(row)\n",
    "                seen_times.add(row.open_time)\n",
    "\n",
    "        print(f\"Before deduplication: {len(all_rows)} records\")\n",
    "        print(f\"After deduplication: {len(unique_rows)} records\")\n",
    "        all_rows = unique_rows\n",
    "        all_rows.sort(key=lambda x: x.open_time) # sort by time\n",
    "    \n",
    "    return all_rows, successful_config\n",
    "\n",
    "print(\"Strategic fetcher function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b18ceb1-88b0-4d37-964f-d46acf3b6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trying BTCUSDT with 1m (1-minute) ===\n",
      "Batch 1: Fetching from 2025-06-08 22:26:02.180000 to 2025-06-09 15:06:02.180000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1749421562180&endTime=1749481562180&timestamp=1749481562180&signature=b09de3abd806a92cfc3abab790ff16613a2f70f4dd313466a51f31792f2ee303\n",
      "Received 1000 records, total: 1000\n",
      "Batch 2: Fetching from 2025-06-08 05:46:02.179000 to 2025-06-08 22:26:02.179000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1749361562179&endTime=1749421562179&timestamp=1749481563139&signature=e68a7e7ca161643eaa5d3502329e0fbdaf2911fbf7f896bc572b2fa0336f868b\n",
      "Received 1000 records, total: 2000\n",
      "Batch 3: Fetching from 2025-06-07 13:06:02.178000 to 2025-06-08 05:46:02.178000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1749301562178&endTime=1749361562178&timestamp=1749481564067&signature=fd7542e7c35f26862f4db62c10b8fec17240fef83a19e65c1fcdee40bc470167\n",
      "Received 1000 records, total: 3000\n",
      "Final count for 1m: 3000 records\n",
      "SUCCESS! Got 3000 records with BTCUSDT 1m\n",
      "Before deduplication: 3000 records\n",
      "After deduplication: 3000 records\n",
      "Total records collected: 3000\n",
      "Configuration used: BTCUSDT with 1m interval (1-minute)\n",
      "You have 3000 records!\n"
     ]
    }
   ],
   "source": [
    "rows, config = fetch_historical_data(symbol=\"BTCUSDT\", target_count=3000)\n",
    "print(f\"Total records collected: {len(rows)}\")\n",
    "\n",
    "if config:\n",
    "    print(f\"Configuration used: {config[0]} with {config[1]} interval ({config[2]})\")\n",
    "else:\n",
    "    print(\"No successful configuration found\")\n",
    "\n",
    "print(f\"You have {len(rows)} records!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e163c25-c2cb-4239-a19f-dfb60b8fba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Spark DataFrame from 3000 records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame created with 3000 records\n",
      "\n",
      "DataFrame Schema:\n",
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_volume: double (nullable = true)\n",
      " |-- num_trades: long (nullable = true)\n",
      " |-- taker_base_volume: double (nullable = true)\n",
      " |-- taker_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_datetime: string (nullable = true)\n",
      " |-- close_datetime: string (nullable = true)\n",
      " |-- price_change: double (nullable = true)\n",
      " |-- price_change_pct: double (nullable = true)\n",
      " |-- high_low_spread: double (nullable = true)\n",
      " |-- high_low_spread_pct: double (nullable = true)\n",
      "\n",
      "+-------+-------------------+---------+---------+---------+---------+-------+-------------------+----------------+\n",
      "|symbol |open_datetime      |open     |high     |low      |close    |volume |price_change       |price_change_pct|\n",
      "+-------+-------------------+---------+---------+---------+---------+-------+-------------------+----------------+\n",
      "|BTCUSDT|2025-06-07 13:07:00|105325.6 |105325.61|105325.0 |105325.0 |6.31362|-0.6000000000058208|-6.0E-4         |\n",
      "|BTCUSDT|2025-06-07 13:08:00|105325.01|105325.01|105309.56|105309.57|3.43534|-15.439999999987776|-0.0147         |\n",
      "|BTCUSDT|2025-06-07 13:09:00|105309.56|105343.24|105309.56|105343.24|5.01871|33.68000000000757  |0.032           |\n",
      "|BTCUSDT|2025-06-07 13:10:00|105343.24|105387.64|105343.24|105387.64|4.8046 |44.39999999999418  |0.0421          |\n",
      "|BTCUSDT|2025-06-07 13:11:00|105387.64|105430.37|105387.63|105419.25|3.31468|31.610000000000582 |0.03            |\n",
      "+-------+-------------------+---------+---------+---------+---------+-------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Date range: 2025-06-07 13:07:00 to 2025-06-09 15:06:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 15:06:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|              open|              high|               low|            close|           volume|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|              3000|              3000|              3000|             3000|             3000|\n",
      "|   mean|105875.21488000001|105889.06840000003|105862.20855999998|105876.0221966667|7.350430646666668|\n",
      "| stddev| 610.8414321745595| 616.7490764571152| 605.9000895352486|611.7122256292652|11.72553956669429|\n",
      "|    min|          105001.0|         105036.04|         104964.14|        105001.01|          0.12756|\n",
      "|    max|         107950.45|          108000.0|         107919.56|        107950.44|         207.1351|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n",
      "Data successfully saved to MongoDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create enhanced Spark DataFrame\n",
    "if len(rows) > 0:\n",
    "    print(f\"\\nCreating Spark DataFrame from {len(rows)} records...\")\n",
    "    spark_df = spark.createDataFrame(rows)\n",
    "    \n",
    "    # add enhanced datetime columns and calculated fields\n",
    "    spark_df = spark_df.withColumn(\"open_datetime\", from_unixtime(spark_df.open_time / 1000)) \\\n",
    "                     .withColumn(\"close_datetime\", from_unixtime(spark_df.close_time / 1000))\n",
    "    \n",
    "    # add useful calculated columns\n",
    "    from pyspark.sql.functions import col, round as spark_round\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"price_change\", col(\"close\") - col(\"open\")) \\\n",
    "                     .withColumn(\"price_change_pct\", spark_round(((col(\"close\") - col(\"open\")) / col(\"open\")) * 100, 4)) \\\n",
    "                     .withColumn(\"high_low_spread\", col(\"high\") - col(\"low\")) \\\n",
    "                     .withColumn(\"high_low_spread_pct\", spark_round(((col(\"high\") - col(\"low\")) / col(\"open\")) * 100, 4))\n",
    "    \n",
    "    record_count = spark_df.count()\n",
    "    print(f\"Spark DataFrame created with {record_count} records\")\n",
    "    \n",
    "    print(f\"\\nDataFrame Schema:\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    # display enhanced data sample\n",
    "    spark_df.select(\n",
    "        \"symbol\", \"open_datetime\", \"open\", \"high\", \"low\", \"close\", \n",
    "        \"volume\", \"price_change\", \"price_change_pct\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    # display stats\n",
    "    date_stats = spark_df.select(\"open_datetime\").agg({\n",
    "        \"open_datetime\": \"min\"\n",
    "    }).collect()[0][0]\n",
    "    date_stats_max = spark_df.select(\"open_datetime\").agg({\n",
    "        \"open_datetime\": \"max\"\n",
    "    }).collect()[0][0]\n",
    "    \n",
    "    print(f\"Date range: {date_stats} to {date_stats_max}\")    \n",
    "    price_stats = spark_df.select(\"open\", \"high\", \"low\", \"close\", \"volume\").describe()\n",
    "    price_stats.show()\n",
    "else:\n",
    "    print(\"No data to create DataFrame\")\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully saved to MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa68957-63e3-4ee1-9685-a5fb54b62feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records verified in MongoDB: 3000\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "|symbol |open_datetime      |open     |close    |volume  |price_change_pct|\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "|BTCUSDT|2025-06-09 02:37:00|105617.76|105603.41|1.65613 |-0.0136         |\n",
      "|BTCUSDT|2025-06-09 02:38:00|105603.42|105627.91|3.97305 |0.0232          |\n",
      "|BTCUSDT|2025-06-09 02:39:00|105627.91|105602.87|2.05486 |-0.0237         |\n",
      "|BTCUSDT|2025-06-09 02:40:00|105602.87|105593.34|0.48263 |-0.009          |\n",
      "|BTCUSDT|2025-06-09 02:41:00|105593.34|105534.88|12.72816|-0.0554         |\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Records fetched: 3000\n",
      "Records in MongoDB: 3000\n",
      "Symbol: BTCUSDT\n",
      "Interval: 1m\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "if len(rows) > 0:    \n",
    "    df_read = spark.read \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .option(\"spark.mongodb.read.connection.uri\", \n",
    "                \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "        .load()\n",
    "    \n",
    "    mongo_count = df_read.count()\n",
    "    print(f\"Records verified in MongoDB: {mongo_count}\")\n",
    "    \n",
    "    # display stored data sample\n",
    "    df_read.select(\n",
    "        \"symbol\", \"open_datetime\", \"open\", \n",
    "        \"close\", \"volume\", \"price_change_pct\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    print(f\"Records fetched: {len(rows)}\")\n",
    "    print(f\"Records in MongoDB: {mongo_count}\")\n",
    "    print(f\"Symbol: {config[0] if config else 'BTCUSDT'}\")\n",
    "    print(f\"Interval: {config[1] if config else 'Various'}\")\n",
    "    # spark.stop()\n",
    "    print(f\"Process completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n Process completed but no data was collected\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
