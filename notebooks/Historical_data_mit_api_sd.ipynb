{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162524b5-1c0f-486f-95f0-6b603f9015e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.11/site-packages (4.13.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.11/site-packages (2.0.1)\n",
      "Collecting hmac\n",
      "  Using cached hmac-20101005.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from hmac) (68.2.2)\n",
      "INFO: pip is looking at multiple versions of hmac to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 20081119\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement hashlib (from hmac) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for hashlib\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv pymongo requests findspark hmac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6763ae2-c192-4307-9c31-252088b976ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully\n",
      "API Key: u1sBBGqY...51Qr01bm\n"
     ]
    }
   ],
   "source": [
    "# import libraries and setup\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import hmac\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "import findspark\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv(dotenv_path=\"/home/jovyan/.env\", override=True)\n",
    "API_KEY = os.getenv(\"BINANCE_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"BINANCE_SECRET_KEY\")\n",
    "\n",
    "print(\"Environment variables loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a433905e-8d69-4942-b402-d6da33d8f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07df8e31-989d-4a7e-b118-e002807ba233;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.1.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      ":: resolution report :: resolve 1867ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.1.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-07df8e31-989d-4a7e-b118-e002807ba233\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/3ms)\n",
      "25/06/03 20:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully\n",
      "Spark version: 3.4.2\n"
     ]
    }
   ],
   "source": [
    "# initialise Spark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BinanceToMongoDB_Enhanced\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \n",
    "            \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \n",
    "            \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f362d86b-2a5d-4c71-90ef-d3c406b691c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced Binance API functions with authentication\n",
    "def create_signature(query_string, secret_key):\n",
    "    \"\"\"Create HMAC SHA256 signature for authenticated requests\"\"\"\n",
    "    return hmac.new(\n",
    "        secret_key.encode('utf-8'),\n",
    "        query_string.encode('utf-8'),\n",
    "        hashlib.sha256\n",
    "    ).hexdigest()\n",
    "\n",
    "def get_authenticated_klines(symbol=\"BTCUSDT\", interval=\"1h\", limit=1000, start_time=None, end_time=None):\n",
    "    \"\"\"Fetch klines with authenticated API for potentially more data\"\"\"\n",
    "    base_url = \"https://api.binance.com\"\n",
    "    endpoint = \"/api/v3/klines\"\n",
    "    \n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"limit\": min(limit, 1000)  # Binance max is 1000\n",
    "    }\n",
    "    \n",
    "    if start_time:\n",
    "        params[\"startTime\"] = int(start_time)\n",
    "    if end_time:\n",
    "        params[\"endTime\"] = int(end_time)\n",
    "    \n",
    "    # add timestamp for authenticated request\n",
    "    params[\"timestamp\"] = int(time.time() * 1000)\n",
    "    # create query string\n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    # create signature\n",
    "    signature = create_signature(query_string, SECRET_KEY)\n",
    "    # add signature to params\n",
    "    params[\"signature\"] = signature\n",
    "\n",
    "    headers = {\n",
    "        \"X-MBX-APIKEY\": API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url + endpoint, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Authenticated request failed, trying public: {e}\")\n",
    "        # fallback to public API\n",
    "        return get_public_klines(symbol, interval, min(limit, 1000), start_time, end_time)\n",
    "\n",
    "def get_public_klines(symbol=\"BTCUSDT\", interval=\"1h\", limit=1000, start_time=None, end_time=None):\n",
    "    \"\"\"Fallback public API function\"\"\"\n",
    "    url = \"https://api.binance.com/api/v3/klines\"\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"limit\": limit,\n",
    "    }\n",
    "    if start_time:\n",
    "        params[\"startTime\"] = int(start_time)\n",
    "    if end_time:\n",
    "        params[\"endTime\"] = int(end_time)\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d455d47-363a-4dca-a483-150c2afa909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic fetcher function defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_historical_data(symbol=\"BTCUSDT\", target_count=3000):\n",
    "    \"\"\"\n",
    "    Strategic approach to get 3000+ records:\n",
    "    1. Try shorter intervals first (more data points)\n",
    "    2. Go back in time systematically\n",
    "    3. Use both authenticated and public APIs\n",
    "    \"\"\"\n",
    "    intervals_to_try = [\n",
    "        (\"1m\", \"1-minute\", 1000 * 60),      # 1000 mins back\n",
    "        (\"3m\", \"3-minute\", 3000 * 60),      # 3000 mins back  \n",
    "        (\"5m\", \"5-minute\", 5000 * 60),      # 5000 mins back\n",
    "        (\"15m\", \"15-minute\", 15000 * 60),   # 15000 mins back\n",
    "        (\"30m\", \"30-minute\", 30000 * 60),   # 30000 mins back\n",
    "        (\"1h\", \"1-hour\", 60000 * 60),       # 60000 mins back\n",
    "    ]\n",
    "    \n",
    "    all_rows = []\n",
    "    successful_config = None\n",
    "    for interval, desc, lookback_minutes in intervals_to_try:\n",
    "        print(f\"\\n=== Trying {symbol} with {interval} ({desc}) ===\")\n",
    "        try:\n",
    "            end_time = int(time.time() * 1000)  # current time in milliseconds\n",
    "            lookback_ms = lookback_minutes * 1000  # convert to milliseconds\n",
    "            current_rows = []\n",
    "            current_time = end_time\n",
    "            fetched_batches = 0\n",
    "            max_batches = 10  # limit to avoid infinite loops\n",
    "            while len(current_rows) < target_count and fetched_batches < max_batches:\n",
    "                # calculate start time for this batch\n",
    "                batch_start = current_time - (1000 * lookback_ms // 1000)  # go back by interval\n",
    "                print(f\"Batch {fetched_batches + 1}: Fetching from {datetime.fromtimestamp(batch_start/1000)} to {datetime.fromtimestamp(current_time/1000)}\")\n",
    "                # attempt authenticated first, then public\n",
    "                try:\n",
    "                    data = get_authenticated_klines(\n",
    "                        symbol=symbol, \n",
    "                        interval=interval, \n",
    "                        limit=1000,\n",
    "                        start_time=batch_start,\n",
    "                        end_time=current_time\n",
    "                    )\n",
    "                except:\n",
    "                    data = get_public_klines(\n",
    "                        symbol=symbol, \n",
    "                        interval=interval, \n",
    "                        limit=1000,\n",
    "                        start_time=batch_start,\n",
    "                        end_time=current_time\n",
    "                    )\n",
    "                if not data or len(data) == 0:\n",
    "                    print(f\"No data received for this time range\")\n",
    "                    break\n",
    "                # convert to rows\n",
    "                for row in data:\n",
    "                    current_rows.append(Row(\n",
    "                        symbol=symbol,\n",
    "                        open_time=int(row[0]),\n",
    "                        open=float(row[1]),\n",
    "                        high=float(row[2]),\n",
    "                        low=float(row[3]),\n",
    "                        close=float(row[4]),\n",
    "                        volume=float(row[5]),\n",
    "                        close_time=int(row[6]),\n",
    "                        quote_volume=float(row[7]),\n",
    "                        num_trades=int(row[8]),\n",
    "                        taker_base_volume=float(row[9]),\n",
    "                        taker_quote_volume=float(row[10]),\n",
    "                        ignore=row[11]\n",
    "                    ))\n",
    "                \n",
    "                print(f\"Received {len(data)} records, total: {len(current_rows)}\")\n",
    "                # move time window back\n",
    "                current_time = batch_start - 1\n",
    "                fetched_batches += 1\n",
    "                # rate limiting\n",
    "                time.sleep(0.2) \n",
    "                # stop if we got less than requested (hit the limit)\n",
    "                if len(data) < 1000:\n",
    "                    print(f\"Reached historical data limit\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Final count for {interval}: {len(current_rows)} records\")\n",
    "            # use configuration (with enough records)\n",
    "            if len(current_rows) >= target_count:\n",
    "                print(f\"SUCCESS! Got {len(current_rows)} records with {symbol} {interval}\")\n",
    "                all_rows = current_rows\n",
    "                successful_config = (symbol, interval, desc)\n",
    "                break\n",
    "                \n",
    "            elif len(current_rows) > len(all_rows):\n",
    "                all_rows = current_rows\n",
    "                successful_config = (symbol, interval, desc)\n",
    "                print(f\"Best so far: {len(current_rows)} records with {symbol} {interval}\")      \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol} {interval}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # remove duplicates based on open_time (regarding overlapping batches)\n",
    "    if all_rows:\n",
    "        seen_times = set()\n",
    "        unique_rows = []\n",
    "        for row in all_rows:\n",
    "            if row.open_time not in seen_times:\n",
    "                unique_rows.append(row)\n",
    "                seen_times.add(row.open_time)\n",
    "\n",
    "        print(f\"Before deduplication: {len(all_rows)} records\")\n",
    "        print(f\"After deduplication: {len(unique_rows)} records\")\n",
    "        all_rows = unique_rows\n",
    "        all_rows.sort(key=lambda x: x.open_time) # sort by time\n",
    "    \n",
    "    return all_rows, successful_config\n",
    "\n",
    "print(\"Strategic fetcher function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5757a8-10bb-4a38-b712-7186608a5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trying BTCUSDT with 1m (1-minute) ===\n",
      "Batch 1: Fetching from 2025-06-03 03:43:29.873000 to 2025-06-03 20:23:29.873000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1748922209873&endTime=1748982209873&timestamp=1748982209873&signature=4f4d3aab69f6a694a8bd46537f38090aefb0515d48bb05e89d324017c1a560cf\n",
      "  Received 1000 records, total: 1000\n",
      "Batch 2: Fetching from 2025-06-02 11:03:29.872000 to 2025-06-03 03:43:29.872000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1748862209872&endTime=1748922209872&timestamp=1748982211021&signature=0b3e82372a639db431db436680bc1b22a9e7b1ecd4cf4d97292861bf515df325\n",
      "  Received 1000 records, total: 2000\n",
      "Batch 3: Fetching from 2025-06-01 18:23:29.871000 to 2025-06-02 11:03:29.871000\n",
      "Authenticated request failed, trying public: 400 Client Error: Bad Request for url: https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1m&limit=1000&startTime=1748802209871&endTime=1748862209871&timestamp=1748982211941&signature=8efc3028b1d62cc6ee14f11952082ebb1a29e75a031c9d75d79aab7e36ae406f\n",
      "  Received 1000 records, total: 3000\n",
      "Final count for 1m: 3000 records\n",
      "SUCCESS! Got 3000 records with BTCUSDT 1m\n",
      "Before deduplication: 3000 records\n",
      "After deduplication: 3000 records\n",
      "Total records collected: 3000\n",
      "Configuration used: BTCUSDT with 1m interval (1-minute)\n",
      "You have 3000 records!\n"
     ]
    }
   ],
   "source": [
    "rows, config = fetch_historical_data(symbol=\"BTCUSDT\", target_count=3000)\n",
    "print(f\"Total records collected: {len(rows)}\")\n",
    "\n",
    "if config:\n",
    "    print(f\"Configuration used: {config[0]} with {config[1]} interval ({config[2]})\")\n",
    "else:\n",
    "    print(\"No successful configuration found\")\n",
    "\n",
    "print(f\"You have {len(rows)} records!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5deec230-3ea6-48f5-a18b-9d7c10e504b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Spark DataFrame from 3000 records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame created with 3000 records\n",
      "\n",
      "DataFrame Schema:\n",
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_volume: double (nullable = true)\n",
      " |-- num_trades: long (nullable = true)\n",
      " |-- taker_base_volume: double (nullable = true)\n",
      " |-- taker_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_datetime: string (nullable = true)\n",
      " |-- close_datetime: string (nullable = true)\n",
      " |-- price_change: double (nullable = true)\n",
      " |-- price_change_pct: double (nullable = true)\n",
      " |-- high_low_spread: double (nullable = true)\n",
      " |-- high_low_spread_pct: double (nullable = true)\n",
      "\n",
      "+-------+-------------------+---------+---------+---------+---------+--------+------------------+----------------+\n",
      "|symbol |open_datetime      |open     |high     |low      |close    |volume  |price_change      |price_change_pct|\n",
      "+-------+-------------------+---------+---------+---------+---------+--------+------------------+----------------+\n",
      "|BTCUSDT|2025-06-01 18:24:00|104760.0 |104812.68|104759.99|104800.08|8.23343 |40.080000000001746|0.0383          |\n",
      "|BTCUSDT|2025-06-01 18:25:00|104800.07|104816.46|104800.07|104816.45|1.22242 |16.379999999990105|0.0156          |\n",
      "|BTCUSDT|2025-06-01 18:26:00|104816.46|104839.16|104816.45|104830.99|2.73734 |14.529999999998836|0.0139          |\n",
      "|BTCUSDT|2025-06-01 18:27:00|104831.0 |104831.0 |104702.93|104744.98|18.50964|-86.02000000000407|-0.0821         |\n",
      "|BTCUSDT|2025-06-01 18:28:00|104744.99|104756.11|104692.55|104692.56|15.39105|-52.43000000000757|-0.0501         |\n",
      "+-------+-------------------+---------+---------+---------+---------+--------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Date range: 2025-06-01 18:24:00 to 2025-06-03 20:23:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:23:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|              open|              high|               low|            close|            volume|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|              3000|              3000|              3000|             3000|              3000|\n",
      "|   mean|105196.58943666669|105218.75056333338|105175.09957333334|     105196.95875| 9.243204983333335|\n",
      "| stddev| 649.3010599436975| 648.9950428082631|  649.891605880653|649.3747482393418|10.526017385876173|\n",
      "|    min|         103688.68|         103794.24|         103659.88|        103688.68|           0.22163|\n",
      "|    max|         106746.26|         106794.67|         106708.94|        106746.27|         122.62273|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# create enhanced Spark DataFrame\n",
    "if len(rows) > 0:\n",
    "    print(f\"\\nCreating Spark DataFrame from {len(rows)} records...\")\n",
    "    spark_df = spark.createDataFrame(rows)\n",
    "    \n",
    "    # add enhanced datetime columns and calculated fields\n",
    "    spark_df = spark_df.withColumn(\"open_datetime\", from_unixtime(spark_df.open_time / 1000)) \\\n",
    "                     .withColumn(\"close_datetime\", from_unixtime(spark_df.close_time / 1000))\n",
    "    \n",
    "    # add useful calculated columns\n",
    "    from pyspark.sql.functions import col, round as spark_round\n",
    "    \n",
    "    spark_df = spark_df.withColumn(\"price_change\", col(\"close\") - col(\"open\")) \\\n",
    "                     .withColumn(\"price_change_pct\", spark_round(((col(\"close\") - col(\"open\")) / col(\"open\")) * 100, 4)) \\\n",
    "                     .withColumn(\"high_low_spread\", col(\"high\") - col(\"low\")) \\\n",
    "                     .withColumn(\"high_low_spread_pct\", spark_round(((col(\"high\") - col(\"low\")) / col(\"open\")) * 100, 4))\n",
    "    \n",
    "    record_count = spark_df.count()\n",
    "    print(f\"Spark DataFrame created with {record_count} records\")\n",
    "    \n",
    "    print(f\"\\nDataFrame Schema:\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    # display enhanced data sample\n",
    "    spark_df.select(\n",
    "        \"symbol\", \"open_datetime\", \"open\", \"high\", \"low\", \"close\", \n",
    "        \"volume\", \"price_change\", \"price_change_pct\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    # display stats\n",
    "    date_stats = spark_df.select(\"open_datetime\").agg({\n",
    "        \"open_datetime\": \"min\"\n",
    "    }).collect()[0][0]\n",
    "    date_stats_max = spark_df.select(\"open_datetime\").agg({\n",
    "        \"open_datetime\": \"max\"\n",
    "    }).collect()[0][0]\n",
    "    \n",
    "    print(f\"Date range: {date_stats} to {date_stats_max}\")    \n",
    "    price_stats = spark_df.select(\"open\", \"high\", \"low\", \"close\", \"volume\").describe()\n",
    "    price_stats.show()\n",
    "else:\n",
    "    print(\"No data to create DataFrame\")\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully saved to MongoDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcc8ef3-5a1b-439f-87f7-71b1823efeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records verified in MongoDB: 3000\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "|symbol |open_datetime      |open     |close    |volume  |price_change_pct|\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "|BTCUSDT|2025-06-01 18:24:00|104760.0 |104800.08|8.23343 |0.0383          |\n",
      "|BTCUSDT|2025-06-01 18:25:00|104800.07|104816.45|1.22242 |0.0156          |\n",
      "|BTCUSDT|2025-06-01 18:26:00|104816.46|104830.99|2.73734 |0.0139          |\n",
      "|BTCUSDT|2025-06-01 18:27:00|104831.0 |104744.98|18.50964|-0.0821         |\n",
      "|BTCUSDT|2025-06-01 18:28:00|104744.99|104692.56|15.39105|-0.0501         |\n",
      "+-------+-------------------+---------+---------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Records fetched: 3000\n",
      "Records in MongoDB: 3000\n",
      "Symbol: BTCUSDT\n",
      "Interval: 1m\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "if len(rows) > 0:    \n",
    "    df_read = spark.read \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .option(\"spark.mongodb.read.connection.uri\", \n",
    "                \"mongodb://crypto_project:dst123@crypto_mongo:27017/cryptobot.historical_data?authSource=admin\") \\\n",
    "        .load()\n",
    "    \n",
    "    mongo_count = df_read.count()\n",
    "    print(f\"Records verified in MongoDB: {mongo_count}\")\n",
    "    \n",
    "    # display stored data sample\n",
    "    df_read.select(\n",
    "        \"symbol\", \"open_datetime\", \"open\", \n",
    "        \"close\", \"volume\", \"price_change_pct\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    print(f\"Records fetched: {len(rows)}\")\n",
    "    print(f\"Records in MongoDB: {mongo_count}\")\n",
    "    print(f\"Symbol: {config[0] if config else 'BTCUSDT'}\")\n",
    "    print(f\"Interval: {config[1] if config else 'Various'}\")\n",
    "    # spark.stop()\n",
    "    print(f\"Process completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n Process completed but no data was collected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233bab0-f4f6-4f50-94fa-a6662751b3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
