# CryptoBot Project

Welcome!
Weâ€™re excited to present **CryptoBot**, our capstone project for the Data Engineer Bootcamp at DataScientest. Our solution applies data engineering best practices, incorporates team agreements, and is tailored to fit the assignment timeline.

Below, weâ€™ll outline our projectâ€™s objectives, main features, and quick setup instructions so you can get started easily.

---

## ðŸš€ Project Overview

**CryptoBot** is a scalable, microservices-based platform designed to collect, store, process, and analyze both historical and real-time cryptocurrency data from Binance. The system delivers processed metrics and visual analytics through a unified API layer and interactive Dash dashboard.

### What is Binance?

[Binance](https://www.binance.com/) is one of the worldâ€™s top cryptocurrency exchanges, ranking in the global top 3 by trading volume.
For this project, we focused on the two most liquid trading pairs:

* **BTC/USDT** (Bitcoin)
* **ETH/USDT** (Ethereum)

---

## ðŸ› ï¸ Key Features

### **Data Extraction**

* **Historical Data:**
  Collects 6 months of 15-minute interval price data using the Binance REST API.
* **Real-Time Data:**
  Streams live market data using the Binance WebSocket API.
* **Incremental Extraction:**
  Automatically fetches only the latest 15-minute intervals each day.

### **Data Pipeline**

* **Batch Extraction:** PySpark for efficient historical data ingestion.
* **Streaming:** Kafka for real-time event streaming.
* **Storage:** MongoDB, with separate collections for historical (`historical_data_15m`) and streaming (`streaming_data_1m`) data.

### **Processing**

* **Analysis:** Jupyter Notebooks for data exploration and initial analysis.
* **Streaming Processing:** Python scripts to manage and process live data.

### **Visualization**

* **Dashboard:** Dash application for interactive, real-time, and historical charts.

### **API Layer**

* **FastAPI:** Provides an interface for the Dash dashboard to retrieve processed historical data directly from MongoDB.

### **Automation & Deployment**

* **Containerization:** All components are Dockerized and orchestrated using Docker Compose.
* **Scheduling:** Cron jobs automate regular historical data extraction tasks.

### **CI/CD**

* **GitHub Actions:**

  * Linting, testing, and Docker image builds for each commit (`ci.yaml`)
  * Automatic deployment of Docker images to DockerHub on release (`release.yaml`)

### **Data Achitecture**

```mermaid
flowchart TD
    A[Binance REST API] --> B[PySpark Extraction]
    B --> C[Preprocessing]
    C --> D[MongoDB<br>historical_data]
    E[Binance WebSocket API] --> F[Kafka Streaming]
    F --> G[Preprocessing]
    G --> H[MongoDB<br>streaming_data]
    D --> I[Processing]
    H --> I
    I --> J[Dashboards]
```
![](./references/ArchichectureProjectOPA.png)


---

##  Project Overview
### 1. Project Structure:
**apr25_bde_int_opa_team_a**
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ Check Mongodb.ipynb
â”‚Â Â  â”œâ”€â”€ Historical_data.ipynb
â”‚Â Â  â”œâ”€â”€ Historical_data_mit_api_sd_v1.ipynb
â”‚Â Â  â””â”€â”€ Kafka.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ references
â”‚Â Â  â”œâ”€â”€ API explanation.md
â”‚Â Â  â”œâ”€â”€ command.md
â”‚Â Â  â”œâ”€â”€ Repo Structure Tutorial.md
â”‚Â Â  â””â”€â”€ Step 1.md
â””â”€â”€ src
    â”œâ”€â”€ collection_admin
    â”‚Â Â  â”œâ”€â”€ data
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ initialize_historical_data.py
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ update_historical_data.py
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ kafka_consumer.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ kafka_producer.py
    â”‚Â Â  â”œâ”€â”€ db
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ mongo_utils.py
    â”‚Â Â  â”œâ”€â”€ docker
    â”‚Â Â  â”‚Â Â  â””â”€â”€ Dockerfile.data_collector
    â”‚   â”‚Â Â  â””â”€â”€ entrypoint.sh
    â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â””â”€â”€ requirements.txt
    â””â”€â”€ api_user
        â”œâ”€â”€ docker
        â”‚Â Â  â””â”€â”€ Dockerfile.dash
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ requirements.txt
        â””â”€â”€ visualization
            â”œâ”€â”€ dash_app.py
            â”œâ”€â”€ dash_stream.py
            â””â”€â”€ __init__.py


### 2. Set up the project

#### 2.1 Environment Variables

Create a `.env` file in the root directory:

```dotenv
MONGO_INITDB_ROOT_USERNAME=your_user
MONGO_INITDB_ROOT_PASSWORD=your_pass
```

Create a `.env` file in src/collection_admin directory:
```dotenv
MONGO_INITDB_ROOT_USERNAME=your_user
MONGO_INITDB_ROOT_PASSWORD=your_pass
MONGO_URI=mongodb://your_user:your_pass@crypto_mongo:27017/cryptobot?authSource=admin
BINANCE_API_KEY=api_key
BINANCE_SECRET_KEY=api_secret
```

Create a `.env` file in src/api_user directory:

```dotenv
MONGO_URI=mongodb://your_user:your_pass@crypto_mongo:27017/cryptobot?authSource=admin
```
#### 2.2 **Launch containers**:

  ```bash
  docker-compose build
  docker-compose up -d
  # It will display 8 containers
  docker ps
  ```
  ![](./references/docker_ps.png)


#### 2.3 Running Data Collector Scripts 

  Seed 3â€“6 months of 15m data: 
  ```bash
  docker exec -it crypto_data_collector python /app/src/collection_admin/data/initialize_historical_data.py
  ```
  Pull only new 15m candles:
  ```bash
  docker exec -it crypto_data_collector python /app/src/collection_admin/data/update_historical_data.py
  ```
  Start 1-minute Kafka producer:
  ```bash
  docker exec -it crypto_data_collector python /app/src/collection_admin/data/kafka_producer.py
  ```
  Start Kafka consumer:
  ```bash
  docker exec -it crypto_data_collector python /app/src/collection_admin/data/kafka_consumer.py
  ```

  ### Services Overview

  | Service             | Description                   | Port  |
  | -----------         | ----------------------------- | ----- |
  | `jupyter` (Legacy)  | PySpark + Jupyter Notebook    | 8888  |
  | `kafka`             | Kafka broker                  | 9092  |
  | `fastapi`           | Dash dashboard                | 8000  |
  | `zookeeper`         | Manages Kafka                 | 2181  |
  | `mongo`             | NoSQL document DB             | 27017 |
  | `dash`              | Dash dashboard                | 8050  |


  ### Data Sources Identified

  | Source                | Access Method             | Data Type                            |
  | --------------------- | ------------------------- | ------------------------------------ |
  | Binance REST API      | `https://api.binance.com` | Market data (prices, trades, klines) |
  | Binance WebSocket API | Real-time JSON stream     | Live market changes                  |

---
#### 2.4 Access services:

* Dash app: [http://localhost:8050](http://localhost:8050)
![](/references/historical_dashboard.png)

* MongoDB (from containers): `crypto_mongo:27017`


## Tools Used

* Python 3.9
* PySpark
* Kafka (Bitnami images)
* MongoDB
* Dash (Plotly)
* Docker & Docker Compose
* FastAPI
* Pytest
* Binance API
* Flake8
* GitHub Actions
* Cron Job
* REST API
* WebSocket

## Authors

* Team A â€“ DataScientest : Bootcamp Data Engineer Project (April 2025)
  * Indira Burga 
  * Katharina Klat
  * Siobhan Doherty

---

## License

Licensed under the [Apache License 2.0](./LICENSE).
